{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rCRl1Bae1bl",
        "outputId": "74baaeb4-8b72-424c-bff4-f02f7d450245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/MyDrive\n",
            "Archive:  /content/MyDrive/MyDrive/v2_Annotations_Train_mscoco.zip\n",
            "  inflating: /content/annotations/v2_mscoco_train2014_annotations.json  \n",
            "Archive:  /content/MyDrive/MyDrive/v2_Questions_Train_mscoco.zip\n",
            "  inflating: /content/questions/v2_OpenEnded_mscoco_train2014_questions.json  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting the drive to access files\n",
        "drive.mount('/content/MyDrive')\n",
        "!unzip /content/MyDrive/MyDrive/v2_Annotations_Train_mscoco.zip -d /content/annotations\n",
        "!unzip /content/MyDrive/MyDrive/v2_Questions_Train_mscoco.zip -d /content/questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2ZVUnRmZfwQY"
      },
      "outputs": [],
      "source": [
        "# Global variable for dimension size\n",
        "GLOBAL_DIM = 11600\n",
        "\n",
        "# Loading image embeddings from a CSV file\n",
        "image_embeddings = pd.read_csv(\"/content/i_embeddings.csv\")\n",
        "# Loading question embeddings using pickle\n",
        "question_embeddings = pickle.load(open(\"/content/q_embeddings.pkl\", \"rb\"))\n",
        "# Extracting vectors from question embeddings\n",
        "question_embeddings_processed = question_embeddings['vec'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v9IfjNQZf0A3"
      },
      "outputs": [],
      "source": [
        "# Reshaping question embeddings to flatten them\n",
        "for i in range(question_embeddings_processed.shape[0]):\n",
        "    question_embeddings_processed[i]=tf.reshape(question_embeddings_processed[i],[question_embeddings_processed[i].shape[1]*768])\n",
        "\n",
        "# Function to align dimensions of question and image embeddings\n",
        "def shrink_dimensions(question,image):\n",
        "    return tf.pad(tf.concat([question,image], 0), tf.constant([[0, GLOBAL_DIM-tf.concat([question,image], 0).shape[0],]]), \"CONSTANT\")\n",
        "\n",
        "# Calculating length of each question\n",
        "question_length = np.array([len(quest) for quest in question_embeddings['question']])\n",
        "\n",
        "# Extracting unique image IDs\n",
        "seen = set()\n",
        "image_ids = np.array([x for x in question_embeddings['image_id'] if x not in seen and (seen.add(x) or True)]) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "91pNQWY6f2OK"
      },
      "outputs": [],
      "source": [
        "# Reading and parsing annotations JSON\n",
        "with open(\"/content/annotations/v2_mscoco_train2014_annotations.json\",'r') as f:\n",
        "    annotations =f.read()\n",
        "annotations=json.loads(annotations)['annotations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8Zxt42amisMm"
      },
      "outputs": [],
      "source": [
        "# Counting occurrences of each answer\n",
        "ans_sorted = defaultdict(lambda: 0)\n",
        "for ann in annotations:\n",
        "    for ans in ann['answers']:\n",
        "        if re.search(r\"[^\\w\\s]\", ans['answer']):\n",
        "            continue\n",
        "        ans_sorted[ans['answer']] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tpgwBOhkisO5"
      },
      "outputs": [],
      "source": [
        "# Sorting answers and checking for '<unk>'\n",
        "ans = sorted(ans_sorted, key=ans_sorted.get, reverse=True)\n",
        "if '<unk>' in ans:\n",
        "    raise ValueError(\"'<unk>' found in answers\")\n",
        "\n",
        "# Saving top 500 answers to a file\n",
        "with open('/content/vocab.txt', 'w') as f:\n",
        "    for answer in ['<unk>'] + ans[:500-1]:\n",
        "        f.write(answer + '\\n')\n",
        "\n",
        "# Regular expression for tokenizing sentences\n",
        "GLOBAL_SPLIT = re.compile(r'(\\W+)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OrsuhZYnisRD"
      },
      "outputs": [],
      "source": [
        "# Class for handling vocabulary and tokenization\n",
        "class VocabDict:\n",
        "    # Private method for loading words from file\n",
        "    def _load_words(self, path_ann):\n",
        "        with open(path_ann, 'r') as f:\n",
        "            return [l.strip() for l in f]\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, vocab_file):\n",
        "        self.word_L = self._load_words(vocab_file)\n",
        "        self.convertToDict = {word: idx for idx, word in enumerate(self.word_L)}\n",
        "        self.tokenToDict = self.convertToDict.get('<unk>')\n",
        "\n",
        "    # Method to convert words to indices\n",
        "    def convertWords(self, word):\n",
        "        return self.convertToDict.get(word, self.tokenToDict)\n",
        "\n",
        "    # Tokenize and index a sentence\n",
        "    def tokenize_and_index(self, sentence):\n",
        "        words = filter(None, [word.strip() for word in GLOBAL_SPLIT.split(sentence.lower())])\n",
        "        return [self.convertWords(word) for word in words]\n",
        "\n",
        "# Creating a vocabulary dictionary from the answers file\n",
        "ans_vocab = VocabDict('/content/vocab.txt').convertToDict\n",
        "answers = np.array(list(ans_vocab.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "m2hN_3ofisTT"
      },
      "outputs": [],
      "source": [
        "# Initializing arrays for storing data\n",
        "x_tensor = []\n",
        "q_id = []\n",
        "\n",
        "# Function to parse image vectors\n",
        "def parse_image_vector(vector_str):\n",
        "        return np.fromstring(vector_str.replace('\\n', '').replace('[', '').replace(']', '').replace('  ', ' '), sep=' ')\n",
        "\n",
        "# Constructing the input tensor by combining question and image features\n",
        "for i in range(question_embeddings.shape[0]):\n",
        "    image_features = image_embeddings.loc[image_embeddings['Image ID'] == question_embeddings.iloc[i]['image_id'], 'Image Vector'].apply(parse_image_vector).tolist()\n",
        "\n",
        "    if not all(array.shape == image_features[0].shape for array in image_features):\n",
        "        print(f\"Error: Different shapes found in image_features at index {i}\")\n",
        "        continue\n",
        "\n",
        "    if question_embeddings_processed[i].shape[0] < 11000:\n",
        "        x_tensor.append(shrink_dimensions(question_embeddings_processed[i], tf.convert_to_tensor(image_features, dtype=tf.float32)[0]))\n",
        "        q_id.append(question_embeddings.iloc[i][\"question_id\"])\n",
        "\n",
        "# Function to find unique answer for a given question ID\n",
        "def unique_answer(question_id):\n",
        "    for answer in annotations:\n",
        "        if answer['question_id']==question_id:\n",
        "            return answer['multiple_choice_answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6AKxKwzFi9pQ"
      },
      "outputs": [],
      "source": [
        "# Converting tensors to numpy arrays and initializing labels\n",
        "for i in range(len(x_tensor)):\n",
        "    x_tensor[i]=x_tensor[i].numpy()\n",
        "x_train=np.array(x_tensor)\n",
        "label=np.zeros(shape=(len(x_tensor),500))\n",
        "\n",
        "# Assigning labels to training data\n",
        "for i in range(len(q_id)):\n",
        "    try:\n",
        "        key=ans_vocab[unique_answer(q_id[i])]\n",
        "        label[i][key]=1\n",
        "    except:\n",
        "        label[i][0]=1\n",
        "label=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1i6S5lZ2i9rU"
      },
      "outputs": [],
      "source": [
        "# Defining the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, input_dim=x_tensor[0].shape[0], activation='relu'))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(500, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Preparing data for training\n",
        "x=x_train\n",
        "y=label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-0e9BZDi9tg",
        "outputId": "82d175f5-d840-4a78-ecd6-10cf002908c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "152/152 [==============================] - 10s 57ms/step - loss: 2.8701 - accuracy: 0.3774\n",
            "Epoch 2/50\n",
            "152/152 [==============================] - 6s 40ms/step - loss: 2.2452 - accuracy: 0.4300\n",
            "Epoch 3/50\n",
            "152/152 [==============================] - 7s 47ms/step - loss: 1.9666 - accuracy: 0.4518\n",
            "Epoch 4/50\n",
            "152/152 [==============================] - 9s 57ms/step - loss: 1.7050 - accuracy: 0.4849\n",
            "Epoch 5/50\n",
            "152/152 [==============================] - 7s 47ms/step - loss: 1.4398 - accuracy: 0.5427\n",
            "Epoch 6/50\n",
            "152/152 [==============================] - 8s 51ms/step - loss: 1.2327 - accuracy: 0.5878\n",
            "Epoch 7/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.9968 - accuracy: 0.6546\n",
            "Epoch 8/50\n",
            "152/152 [==============================] - 7s 47ms/step - loss: 0.8112 - accuracy: 0.7105\n",
            "Epoch 9/50\n",
            "152/152 [==============================] - 7s 45ms/step - loss: 0.6736 - accuracy: 0.7568\n",
            "Epoch 10/50\n",
            "152/152 [==============================] - 6s 37ms/step - loss: 0.5543 - accuracy: 0.8010\n",
            "Epoch 11/50\n",
            "152/152 [==============================] - 8s 52ms/step - loss: 0.4533 - accuracy: 0.8428\n",
            "Epoch 12/50\n",
            "152/152 [==============================] - 6s 40ms/step - loss: 0.3848 - accuracy: 0.8620\n",
            "Epoch 13/50\n",
            "152/152 [==============================] - 6s 39ms/step - loss: 0.3009 - accuracy: 0.8973\n",
            "Epoch 14/50\n",
            "152/152 [==============================] - 8s 53ms/step - loss: 0.2798 - accuracy: 0.9016\n",
            "Epoch 15/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.2117 - accuracy: 0.9279\n",
            "Epoch 16/50\n",
            "152/152 [==============================] - 7s 45ms/step - loss: 0.2226 - accuracy: 0.9252\n",
            "Epoch 17/50\n",
            "152/152 [==============================] - 7s 47ms/step - loss: 0.1718 - accuracy: 0.9425\n",
            "Epoch 18/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.1824 - accuracy: 0.9375\n",
            "Epoch 19/50\n",
            "152/152 [==============================] - 8s 50ms/step - loss: 0.1454 - accuracy: 0.9534\n",
            "Epoch 20/50\n",
            "152/152 [==============================] - 7s 43ms/step - loss: 0.1219 - accuracy: 0.9610\n",
            "Epoch 21/50\n",
            "152/152 [==============================] - 6s 37ms/step - loss: 0.1028 - accuracy: 0.9680\n",
            "Epoch 22/50\n",
            "152/152 [==============================] - 8s 55ms/step - loss: 0.1139 - accuracy: 0.9642\n",
            "Epoch 23/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.1414 - accuracy: 0.9521\n",
            "Epoch 24/50\n",
            "152/152 [==============================] - 6s 42ms/step - loss: 0.1191 - accuracy: 0.9596\n",
            "Epoch 25/50\n",
            "152/152 [==============================] - 8s 50ms/step - loss: 0.1156 - accuracy: 0.9640\n",
            "Epoch 26/50\n",
            "152/152 [==============================] - 6s 37ms/step - loss: 0.1230 - accuracy: 0.9599\n",
            "Epoch 27/50\n",
            "152/152 [==============================] - 7s 47ms/step - loss: 0.0914 - accuracy: 0.9708\n",
            "Epoch 28/50\n",
            "152/152 [==============================] - 7s 45ms/step - loss: 0.0808 - accuracy: 0.9742\n",
            "Epoch 29/50\n",
            "152/152 [==============================] - 6s 37ms/step - loss: 0.1361 - accuracy: 0.9562\n",
            "Epoch 30/50\n",
            "152/152 [==============================] - 8s 53ms/step - loss: 0.1169 - accuracy: 0.9620\n",
            "Epoch 31/50\n",
            "152/152 [==============================] - 6s 39ms/step - loss: 0.1082 - accuracy: 0.9644\n",
            "Epoch 32/50\n",
            "152/152 [==============================] - 6s 40ms/step - loss: 0.0860 - accuracy: 0.9706\n",
            "Epoch 33/50\n",
            "152/152 [==============================] - 8s 52ms/step - loss: 0.0385 - accuracy: 0.9882\n",
            "Epoch 34/50\n",
            "152/152 [==============================] - 6s 37ms/step - loss: 0.0367 - accuracy: 0.9910\n",
            "Epoch 35/50\n",
            "152/152 [==============================] - 7s 45ms/step - loss: 0.0341 - accuracy: 0.9918\n",
            "Epoch 36/50\n",
            "152/152 [==============================] - 7s 47ms/step - loss: 0.1441 - accuracy: 0.9542\n",
            "Epoch 37/50\n",
            "152/152 [==============================] - 6s 37ms/step - loss: 0.1733 - accuracy: 0.9469\n",
            "Epoch 38/50\n",
            "152/152 [==============================] - 8s 50ms/step - loss: 0.1014 - accuracy: 0.9681\n",
            "Epoch 39/50\n",
            "152/152 [==============================] - 6s 42ms/step - loss: 0.0489 - accuracy: 0.9853\n",
            "Epoch 40/50\n",
            "152/152 [==============================] - 6s 41ms/step - loss: 0.0415 - accuracy: 0.9872\n",
            "Epoch 41/50\n",
            "152/152 [==============================] - 8s 54ms/step - loss: 0.1041 - accuracy: 0.9646\n",
            "Epoch 42/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.1225 - accuracy: 0.9626\n",
            "Epoch 43/50\n",
            "152/152 [==============================] - 7s 44ms/step - loss: 0.0989 - accuracy: 0.9695\n",
            "Epoch 44/50\n",
            "152/152 [==============================] - 7s 48ms/step - loss: 0.0720 - accuracy: 0.9772\n",
            "Epoch 45/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.0725 - accuracy: 0.9763\n",
            "Epoch 46/50\n",
            "152/152 [==============================] - 8s 54ms/step - loss: 0.0480 - accuracy: 0.9863\n",
            "Epoch 47/50\n",
            "152/152 [==============================] - 7s 44ms/step - loss: 0.0679 - accuracy: 0.9795\n",
            "Epoch 48/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.0669 - accuracy: 0.9809\n",
            "Epoch 49/50\n",
            "152/152 [==============================] - 8s 55ms/step - loss: 0.0880 - accuracy: 0.9715\n",
            "Epoch 50/50\n",
            "152/152 [==============================] - 6s 38ms/step - loss: 0.0545 - accuracy: 0.9816\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "history = model.fit(x,y, epochs=50, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jaZNtRKpisVo"
      },
      "outputs": [],
      "source": [
        "# Function to predict the answer\n",
        "def pred_ans(i):\n",
        "    image_features = image_embeddings[image_embeddings['Image ID'] == question_embeddings.iloc[i]['image_id']]['Image Vector'].apply(lambda x:\n",
        "                           np.fromstring(\n",
        "                           x.replace('\\n', '')\n",
        "                            .replace('[', '')\n",
        "                            .replace(']', '')\n",
        "                            .replace('  ', ' '), sep=' ')).tolist()\n",
        "\n",
        "    if not all(array.shape == image_features[0].shape for array in image_features):\n",
        "        print(f\"Error: Different shapes found in image_features at index {i}\")\n",
        "        return None\n",
        "\n",
        "    image = tf.convert_to_tensor(image_features, dtype=tf.float32)\n",
        "    question = question_embeddings_processed[i]\n",
        "    t = shrink_dimensions(question, image[0])\n",
        "    t = t.numpy()\n",
        "    t = np.reshape(t, (1, GLOBAL_DIM))\n",
        "    predictions = model.predict(t)\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "    return predicted_class\n",
        "\n",
        "# Creating lists for key-value pairs of answers\n",
        "key_list = list(ans_vocab.keys())\n",
        "val_list = list(ans_vocab.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9zXlZprjE75"
      },
      "outputs": [],
      "source": [
        "# Unzipping the image dataset\n",
        "!unzip /content/MyDrive/MyDrive/train2014.zip -d /content/train2014"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrlqAtrSisX9"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve VQA details for a given question ID\n",
        "def get_vqa_details(question_id):\n",
        "    if question_id >= len(question_embeddings):\n",
        "        return \"Question ID is out of range.\"\n",
        "\n",
        "    image_id = question_embeddings.iloc[question_id]['image_id']\n",
        "    question = question_embeddings.iloc[question_id]['question']\n",
        "\n",
        "    # Constructing the image path\n",
        "    image_path_ann = f'/content/train2014/train2014/COCO_train2014_{str(image_id).zfill(12)}.jpg'\n",
        "\n",
        "    # Predicting the answer\n",
        "    predicted_answer_id = pred_ans(question_id)\n",
        "    if predicted_answer_id is None:\n",
        "        predicted_answer = \"Error in prediction\"\n",
        "    else:\n",
        "        predicted_answer = key_list[val_list.index(predicted_answer_id[0])]\n",
        "\n",
        "    # Retrieving the actual answer\n",
        "    actual_answer = unique_answer(question_embeddings.iloc[question_id][\"question_id\"])\n",
        "\n",
        "    return image_path_ann, question, predicted_answer, actual_answer\n",
        "\n",
        "# Displaying the image and VQA details\n",
        "from IPython.display import Image, display\n",
        "pred_id = 11\n",
        "image_path, question, predicted_answer, actual_answer = get_vqa_details(pred_id)\n",
        "print(f\"Image Path: {image_path}\")\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Predicted Answer: {predicted_answer}\")\n",
        "print(f\"Actual Answer: {actual_answer}\")\n",
        "display(Image(filename=image_path))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
